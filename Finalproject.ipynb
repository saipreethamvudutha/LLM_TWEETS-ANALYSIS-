{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0914b193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           user_name  \\\n",
      "0                                         Walee MENA   \n",
      "1                                            Dataiku   \n",
      "2  NMLT点头形象联系/跑分/灰产/赚钱/项目/副业/偏门/话费/迷药/炒币/NFT/合约/电...   \n",
      "3                                    Lithium Systems   \n",
      "4                            Paramendra Kumar Bhagat   \n",
      "\n",
      "                                                text  \\\n",
      "0  #OpenAI has revealed its plan to launch #ChatG...   \n",
      "1  What are #LargeLanguageModels, how are they de...   \n",
      "2  apodecisionacious\\natappear\\nhe \\n#黑客 #合约 #Cha...   \n",
      "3  Business owner? Stay informed with the latest ...   \n",
      "4  ChatGPT: Motorbike For The Mind (13) #ChatGPT ...   \n",
      "\n",
      "               user_location  \\\n",
      "0                        UAE   \n",
      "1               New York, NY   \n",
      "2               Fayetteville   \n",
      "3  Based in Central Scotland   \n",
      "4                         NY   \n",
      "\n",
      "                                    user_description  \\\n",
      "0  OFFICIALLY IN MENA! We are the region's larges...   \n",
      "1  Dataiku is the only AI platform that connects ...   \n",
      "2  TG：https://t.co/C2kHIu7BKg 官网：https://t.co/56a...   \n",
      "3  Lithium Systems Limited offer I.T and Telecoms...   \n",
      "4  CEO Coach/Kajabi Instructor: ChatGPT Literacy ...   \n",
      "\n",
      "                user_created user_followers user_friends user_favourites  \\\n",
      "0  2022-11-16 12:26:23+00:00         1019.0          2.0               0   \n",
      "1  2012-09-11 06:27:36+00:00        23272.0        692.0            6725   \n",
      "2  2023-05-11 08:06:25+00:00            2.0          1.0               0   \n",
      "3  2009-10-12 13:14:28+00:00          179.0        306.0             454   \n",
      "4  2009-01-25 18:32:43+00:00        32572.0      35920.0           15516   \n",
      "\n",
      "  user_verified                       date  \\\n",
      "0         False  2023-06-07 12:01:00+00:00   \n",
      "1         False  2023-06-07 12:00:59+00:00   \n",
      "2         False  2023-06-07 12:00:25+00:00   \n",
      "3         False  2023-06-07 12:00:25+00:00   \n",
      "4         False  2023-06-07 12:00:22+00:00   \n",
      "\n",
      "                                            hashtags           source  \n",
      "0                              ['OpenAI', 'ChatGPT']  Twitter Web App  \n",
      "1                            ['LargeLanguageModels']          HubSpot  \n",
      "2                            ['黑客', '合约', 'ChatGPT']  Twitter Web App  \n",
      "3                                                NaN   Hootsuite Inc.  \n",
      "4  ['ChatGPT', 'GPT4', 'AI', 'ArtificialIntellige...   Hootsuite Inc.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define column names and their corresponding data types\n",
    "columns = {\n",
    "    \"user_name\": str,\n",
    "    \"text\": str,\n",
    "    \"user_location\": str,\n",
    "    \"user_description\": str,\n",
    "    \"user_created\": str,\n",
    "    \"user_followers\": str,\n",
    "    \"user_friends\": str,\n",
    "    \"user_favourites\": str,\n",
    "    \"user_verified\": str,\n",
    "    \"date\": str,\n",
    "    \"hashtags\": str,\n",
    "    \"source\": str\n",
    "}\n",
    "\n",
    "# Load the dataset with specified column names and data types\n",
    "file_path = '/Users/saipreethamvudutha/Downloads/tweets.csv'\n",
    "df = pd.read_csv(file_path, names=columns.keys(), dtype=columns, skiprows=1)\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "print(df.head())\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Clean the 'text' column (removing special characters and converting to lowercase)\n",
    "df['text'] = df['text'].str.replace('[^\\w\\s]', '').str.lower()\n",
    "\n",
    "# Save the cleaned data to a new CSV file without index\n",
    "cleaned_file_path = '/Users/saipreethamvudutha/Downloads/cleaned_tweets.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8715c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           user_name  \\\n",
      "0                                         Walee MENA   \n",
      "1                                            Dataiku   \n",
      "2  NMLT点头形象联系/跑分/灰产/赚钱/项目/副业/偏门/话费/迷药/炒币/NFT/合约/电...   \n",
      "3                                    Lithium Systems   \n",
      "4                            Paramendra Kumar Bhagat   \n",
      "\n",
      "                                                text  \\\n",
      "0  #OpenAI has revealed its plan to launch #ChatG...   \n",
      "1  What are #LargeLanguageModels, how are they de...   \n",
      "2  apodecisionacious\\natappear\\nhe \\n#黑客 #合约 #Cha...   \n",
      "3  Business owner? Stay informed with the latest ...   \n",
      "4  ChatGPT: Motorbike For The Mind (13) #ChatGPT ...   \n",
      "\n",
      "               user_location  \\\n",
      "0                        UAE   \n",
      "1               New York, NY   \n",
      "2               Fayetteville   \n",
      "3  Based in Central Scotland   \n",
      "4                         NY   \n",
      "\n",
      "                                    user_description  \\\n",
      "0  OFFICIALLY IN MENA! We are the region's larges...   \n",
      "1  Dataiku is the only AI platform that connects ...   \n",
      "2  TG：https://t.co/C2kHIu7BKg 官网：https://t.co/56a...   \n",
      "3  Lithium Systems Limited offer I.T and Telecoms...   \n",
      "4  CEO Coach/Kajabi Instructor: ChatGPT Literacy ...   \n",
      "\n",
      "                user_created user_followers user_friends user_favourites  \\\n",
      "0  2022-11-16 12:26:23+00:00         1019.0          2.0               0   \n",
      "1  2012-09-11 06:27:36+00:00        23272.0        692.0            6725   \n",
      "2  2023-05-11 08:06:25+00:00            2.0          1.0               0   \n",
      "3  2009-10-12 13:14:28+00:00          179.0        306.0             454   \n",
      "4  2009-01-25 18:32:43+00:00        32572.0      35920.0           15516   \n",
      "\n",
      "  user_verified                       date  \\\n",
      "0         False  2023-06-07 12:01:00+00:00   \n",
      "1         False  2023-06-07 12:00:59+00:00   \n",
      "2         False  2023-06-07 12:00:25+00:00   \n",
      "3         False  2023-06-07 12:00:25+00:00   \n",
      "4         False  2023-06-07 12:00:22+00:00   \n",
      "\n",
      "                                            hashtags           source  \n",
      "0                              ['OpenAI', 'ChatGPT']  Twitter Web App  \n",
      "1                            ['LargeLanguageModels']          HubSpot  \n",
      "2                            ['黑客', '合约', 'ChatGPT']  Twitter Web App  \n",
      "3                                                NaN   Hootsuite Inc.  \n",
      "4  ['ChatGPT', 'GPT4', 'AI', 'ArtificialIntellige...   Hootsuite Inc.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define column names and their corresponding data types\n",
    "columns = {\n",
    "    \"user_name\": str,\n",
    "    \"text\": str,\n",
    "    \"user_location\": str,\n",
    "    \"user_description\": str,\n",
    "    \"user_created\": str,\n",
    "    \"user_followers\": str,\n",
    "    \"user_friends\": str,\n",
    "    \"user_favourites\": str,\n",
    "    \"user_verified\": str,\n",
    "    \"date\": str,\n",
    "    \"hashtags\": str,\n",
    "    \"source\": str\n",
    "}\n",
    "\n",
    "# Load the dataset with specified column names and data types\n",
    "file_path = '/Users/saipreethamvudutha/Downloads/tweets.csv'\n",
    "df = pd.read_csv(file_path, names=columns.keys(), dtype=columns, skiprows=1)\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "print(df.head())\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Define a function to remove Chinese characters from a string\n",
    "def remove_chinese(text):\n",
    "    return re.sub(r'[^\\x00-\\x7F\\u4E00-\\u9FFF]+', '', str(text))\n",
    "\n",
    "# Columns where Chinese characters might be present\n",
    "columns_to_clean = [\"user_name\", \"text\", \"user_location\", \"user_description\", \"user_created\",\"user_followers\",\"user_friends\",\"user_favourites\",\"user_verified\",\"hashtags\", \"source\"]\n",
    "\n",
    "# Clean Chinese characters from specific columns\n",
    "for col in columns_to_clean:\n",
    "    df[col] = df[col].apply(remove_chinese)\n",
    "\n",
    "# Save the cleaned data to a new CSV file without index\n",
    "cleaned_file_path = '/Users/saipreethamvudutha/Downloads/cleaned_tweets.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce0fef4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           user_name  \\\n",
      "0                                         Walee MENA   \n",
      "1                                            Dataiku   \n",
      "2  NMLT点头形象联系/跑分/灰产/赚钱/项目/副业/偏门/话费/迷药/炒币/NFT/合约/电...   \n",
      "3                                    Lithium Systems   \n",
      "4                            Paramendra Kumar Bhagat   \n",
      "\n",
      "                                                text  \\\n",
      "0  #OpenAI has revealed its plan to launch #ChatG...   \n",
      "1  What are #LargeLanguageModels, how are they de...   \n",
      "2  apodecisionacious\\natappear\\nhe \\n#黑客 #合约 #Cha...   \n",
      "3  Business owner? Stay informed with the latest ...   \n",
      "4  ChatGPT: Motorbike For The Mind (13) #ChatGPT ...   \n",
      "\n",
      "               user_location  \\\n",
      "0                        UAE   \n",
      "1               New York, NY   \n",
      "2               Fayetteville   \n",
      "3  Based in Central Scotland   \n",
      "4                         NY   \n",
      "\n",
      "                                    user_description  \\\n",
      "0  OFFICIALLY IN MENA! We are the region's larges...   \n",
      "1  Dataiku is the only AI platform that connects ...   \n",
      "2  TG：https://t.co/C2kHIu7BKg 官网：https://t.co/56a...   \n",
      "3  Lithium Systems Limited offer I.T and Telecoms...   \n",
      "4  CEO Coach/Kajabi Instructor: ChatGPT Literacy ...   \n",
      "\n",
      "                user_created user_followers user_friends user_favourites  \\\n",
      "0  2022-11-16 12:26:23+00:00         1019.0          2.0               0   \n",
      "1  2012-09-11 06:27:36+00:00        23272.0        692.0            6725   \n",
      "2  2023-05-11 08:06:25+00:00            2.0          1.0               0   \n",
      "3  2009-10-12 13:14:28+00:00          179.0        306.0             454   \n",
      "4  2009-01-25 18:32:43+00:00        32572.0      35920.0           15516   \n",
      "\n",
      "  user_verified                       date  \\\n",
      "0         False  2023-06-07 12:01:00+00:00   \n",
      "1         False  2023-06-07 12:00:59+00:00   \n",
      "2         False  2023-06-07 12:00:25+00:00   \n",
      "3         False  2023-06-07 12:00:25+00:00   \n",
      "4         False  2023-06-07 12:00:22+00:00   \n",
      "\n",
      "                                            hashtags           source  \n",
      "0                              ['OpenAI', 'ChatGPT']  Twitter Web App  \n",
      "1                            ['LargeLanguageModels']          HubSpot  \n",
      "2                            ['黑客', '合约', 'ChatGPT']  Twitter Web App  \n",
      "3                                                NaN   Hootsuite Inc.  \n",
      "4  ['ChatGPT', 'GPT4', 'AI', 'ArtificialIntellige...   Hootsuite Inc.  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define column names and their corresponding data types\n",
    "columns = {\n",
    "    \"user_name\": str,\n",
    "    \"text\": str,\n",
    "    \"user_location\": str,\n",
    "    \"user_description\": str,\n",
    "    \"user_created\": str,\n",
    "    \"user_followers\": str,\n",
    "    \"user_friends\": str,\n",
    "    \"user_favourites\": str,\n",
    "    \"user_verified\": str,\n",
    "    \"date\": str,\n",
    "    \"hashtags\": str,\n",
    "    \"source\": str\n",
    "}\n",
    "\n",
    "# Load the dataset with specified column names and data types\n",
    "file_path = '/Users/saipreethamvudutha/Downloads/tweets.csv'\n",
    "df = pd.read_csv(file_path, names=columns.keys(), dtype=columns, skiprows=1)\n",
    "\n",
    "# Display the first few rows to understand the structure of the data\n",
    "print(df.head())\n",
    "\n",
    "# Handle missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Remove duplicates\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Define a function to remove Chinese characters from a string\n",
    "def remove_chinese(text):\n",
    "    return re.sub(r'[\\u4E00-\\u9FFF]+', '', str(text))\n",
    "\n",
    "# Define a function to remove URLs from a string\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+', '', str(text))\n",
    "\n",
    "# Define a function to eliminate '////' from user_name\n",
    "def remove_slashes(text):\n",
    "    return text.replace('////', '')\n",
    "\n",
    "# Columns where Chinese characters might be present\n",
    "columns_to_clean = [\"user_name\", \"text\", \"user_location\", \"user_description\", \"hashtags\", \"source\"]\n",
    "\n",
    "# Clean Chinese characters and URLs from specific columns\n",
    "for col in columns_to_clean:\n",
    "    df[col] = df[col].apply(remove_chinese)\n",
    "    df[col] = df[col].apply(remove_urls)\n",
    "\n",
    "# Remove '////' from user_name column\n",
    "df['user_name'] = df['user_name'].apply(remove_slashes)\n",
    "\n",
    "# Save the cleaned data to a new CSV file without index\n",
    "cleaned_file_path = '/Users/saipreethamvudutha/Downloads/cleaned_tweets.csv'\n",
    "df.to_csv(cleaned_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28beaef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
